################################
# Learning problem  
################################
# number of training data elements
ntraining = 5000

# number of validation data elements
nvalidation = 200

# number of features in training and validation data 
nfeatures = 2

# number of classes
nclasses = 5

# number of channels
nchannels = 8

# number of layers 
nlayers = 32

# final time
T = 10.0

# Activation function ("ReLu" or "tanh")
activation = tanh

# Apply opening layer sigma(KY+mu) ("YES" or "NO"). If NO: expands data to channels using zeros.
openinglayer = NO;

################################
#BRAID 
################################

# braid: coarsening factor
braid_cfactor = 2

# braid: maximum number of levels 
braid_maxlevels = 1

# braid: maximum number of iterations
braid_maxiter = 10

# braid: absolute tolerance
braid_abstol = 1e-10

# braid: absolute adjoint tolerance
braid_adjtol = 1e-6

# braid: printlevel
braid_printlevel = 1

# braid: access level
braid_accesslevel = 0

# braid: skip work on downcycle?
braid_setskip = 0

####################################
#Optimization
####################################

# relaxation param for theta tikhonov term
gamma_theta_tik = 1e-2

# relaxation param for theta time-derivative term
gamma_theta_ddt = 1e-0

# relaxation term for classifier
gamma_class = 1e-5

# initial stepsize
stepsize = 1.0

# maximum number of optimization iterations
optim_maxiter = 500

# absolute stopping criterion for the gradient norm
gtol = 1e-4

# maximum number of linesearch iterations
ls_maxiter = 20

# factor for modifying the stepsize within a linesearch iteration
ls_factor = 0.5

# factor for scaling initial design values
design_init = 1e-3
